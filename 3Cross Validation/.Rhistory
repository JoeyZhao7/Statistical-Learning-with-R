library(IALR)
library(ISLR)
install.packages(glmnet)
install.packages(glmnet)
install.packages(glmnet)
install.packages(glmnet)
install.packages("glmnet")
View(hitters)
View(Hitters)
library(glmnet)
do=na.omit(Hitters)
x=model.matrix(Salary~.,d0)[,-1]
d0=na.omit(Hitters)
x=model.matrix(Salary~.,d0)[,-1]
y=d0$Salary
a=seq(from=10,to=-2.length=100)
a=seq(from=10,to=-2,length=100)
a
grid=10^a
ridge.mod=glmnet(x,y,alpha = 0,lambda = grid)
coef(ridge.mod)
cc=coef(ridge.mod)
View(cc)
predict(ridge.mod,as=54.5,type=""coefficients)
predict(ridge.mod,as=54.5,type="coefficients")
predict(ridge.mod,s=54.5,type="coefficients")
set.seed(1)
train=sample(n,n/2)
n=nrow(x)
set.seed(1)
train=sample(n,n/2)
test=(-train)
y.test=y[test]
library(MASS)
d0=Cars93
d1 = subset(d0,select=c(MPG.city,Cylinders,EngineSize,Horsepower,RPM,Passengers,Weight))
d1$Cylinders=as.numeric(d1$Cylinders)
library(leaps)
models=regsubsets(MPG.city~.,d1)
summary(models)
m0 = lm(MPG.city~EngineSize+Horsepower+RPM+Weight,d1)
newval=data.frame(Cylinders=4,EngineSize=2.3,Horsepower=200,RPM=5500,Passengers=4,Weight=2950)
predict(m0,newval)
models=regsubsets(MPG.city~.,d1)
summary(models)
a=summary(models)
a$adjr2
m0 = lm(MPG.city~EngineSize+Horsepower+RPM+Weight,d1)
newval=data.frame(MPG.city=0,Cylinders=4,EngineSize=2.3,Horsepower=200,RPM=5500,Passengers=4,Weight=2950)
predict.regsubsets(models,newval,id=4)
library(leaps)
models=regsubsets(MPG.city~.,d1)
a=summary(models)
a$adjr2
m0 = lm(MPG.city~EngineSize+Horsepower+RPM+Weight,d1)
newval=data.frame(Cylinders=4,EngineSize=2.3,Horsepower=200,RPM=5500,Passengers=4,Weight=2950)
predict(m0,newval)
newval=data.frame(MPG.city=0,Cylinders=4,EngineSize=2.3,Horsepower=200,RPM=5500,Passengers=4,Weight=2950)
predict.regsubsets(models,newval,id=4)
predict.regsubsets <- function(object, newdata, id, ...)
{
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi = coef(object, id = id)
xvars <- names(coefi)
mat[, xvars]%*%coefi
}
predict.regsubsets(models,newval,id=4)
n = nrow(d1)
n/2
ceiling(n/2)
set.seed(12)
train = sample(1:n,47)   # training row numbers
train
n
models = regsubsets(MPG.city~.,d1train)
d1train = d1[train,]
d1test = d1[-train,]
models = regsubsets(MPG.city~.,d1train)
help(rep)
mspe = rep(0,6)
mspe
for(i in 1:6)
{
yhat = predict.regsubsets(models,d1test,id=i)
mspe[i] = mean((y-yhat)^2)
}
mspe
y = d1test$MPG.city
for(i in 1:6)
{
yhat = predict.regsubsets(models,d1test,id=i)
mspe[i] = mean((y-yhat)^2)
}
mspe
library(PASWR2)   # dataset
library(leaps)    # regsubsets()
d0 = HSWRESTLER
head(d0)
ig = c(22,27,32,35,60)
d1 = d0[-ig,1:7]
head(d1)
n = nrow(d1)
k = 5   # folds
set.seed(5)
folds = sample(k,n,replace=T)
folds
head(folds)
[1] 2 4 5 2 1 4
table(folds)
folds
length(folds)
mspe = matrix(0,k,6)   # 5-by-6 matrix
mspe
b=d1[folds==1,]
dim(b)
b
y=d1$hwfat[folds==1]
y
library(ISLR)   # Auto dataframe
train=sample(x=1:10,size=6)
set.seed(1)
train = sample(392,size=196)
m1 = lm(mpg~horsepower,Auto,subset=train)
attach(Auto)
train
m2 = lm(mpg~poly(horsepower,2),Auto,subset=train)
m2
library(boot)  # cv.glm()
m1 = lm(mpg~horsepower,Auto)
coef(m1)
glm1 = glm(mpg~horsepower,Auto)
coef(m1)
glm1 = glm(mpg~horsepower,data=Auto)
help(glm)
cverror = rep(0,5)  # vector to store mspes
for (i in 1:5)
{
models = glm(mpg~poly(horsepower,i),data=Auto)
cverror[i] = cv.glm(Auto,models)$delta[1]
}
cverror
cverror$delta
cverr=cv.glm(Auto,glm1)
summary(cverr)
cverr$delta    # MSPE or CV from LOOCV
library(ISLR)
dim(Hitters)
View(Hitters)
View(Hitters)
library(glmnet)
d0=na.omit(Hitters)
x=model.matrix(Salary~.,d0)[,-1]
a=matrix(0.5,0.5,0,0;1,1,1,1)
c=(1,2)
x=c(0.5,0,5,0,0,0.5,0,0.5,0,0.5,0,0,0.5,0,0,0,1)
m=matrix(c,nrow = 4,ncol = 4)
m=matrix(1:16,nrow = 4,ncol = 4)
matrix
m
m=matrix(X,nrow = 4,ncol = 4,byrow = F)
m=matrix(x,nrow = 4,ncol = 4,byrow = F)
x=c(0.5,0.5,0,0,0.5,0,0.5,0,0.5,0,0,0.5,0,0,0,1)
m=matrix(x,nrow = 4,ncol = 4,byrow = F)
m
m^10
m^2
x=c(0.5,0.5,0,0,0.5,0,0.5,0,0.5,0,0,0.5,0,0,0,1)
m=matrix(x,nrow = 4,ncol = 4,byrow = T)
m
m^10
m^2
value=m
for(i=1 to 9)
for(i in 1 to 9)
for(i in 1:9)
{value=value%*%m}
value=m
for(i in 1:9)
value=m
for (i in 1:9)
{
value=value%*%m
}
value
value=m
for (i in 1:1)
{
value=value%*%m
}
value
m
library(ISLR)
install.packages("glmnet")
install.packages("glmnet")
library(glmnet)
d0=na.omit(Hitters)
x=model.matrix(Salary~.,d0)[,-1]
x
view(x)
View(x)
View(d0)
y=d0$Salary
a=seq(from=10,to=-2,length=100)
a
grid=10^a
ridge.mod=glmnet(x,y,alpha = 0,lambda = grid)
coef(ridge.mod)
predict(ridge.mod,s=54.5,type="coefficients")
help(predict)
m
value=m
for (i in 1:1)
m
value=m
for (i in 1:9)
{
value=value%*%m
}
value
sqrt(sum(coef(ridge.mod)[-1,50]^2))
coef(ridge.mod)
ll=coef(ridge.mod)
View(ll)
ll=coef(ridge.mod)[-1,50]
ll
ll=coef(ridge.mod)[,50]
ll
library(ISLR)   # Auto dataframe
set.seed(1)
train = sample(392,size=196)
train
m1 = lm(mpg~horsepower,Auto,subset=train)
attach(Auto)
res = (mpg-predict(m1,Auto))[-train]^2
cverr=cv.glm(Auto,glm1)
library(boot)  # cv.glm()
cverr=cv.glm(Auto,glm1)
summary(cverr)
cverr$delta    # MSPE or CV from LOOCV
help("cv.glmnet")
help("glmnet")
help("CV.glmnet")
library(PASWR2)
View(HSWRESTLER)
d0=HSWRESTLER
d1=subset(d0,c(hwfat,age,ht,wt,abs,triceps,subscap))
d1=subset(d0,select=c(hwfat,age,ht,wt,abs,triceps,subscap))
d1
x=model.matrix(d1)[,-1]
x=model.matrix(d1)
x=model.matrix(hwfat~.,d1)
x=model.matrix(hwfat~.,d1)[-1,]
x
x=model.matrix(hwfat~.,d1)[,-1]
x
y=d1$hwfat
set.seed(1)
nrow(d1)
n=nrow(d1)
train=sample(2,n/2)
train=sample(2,n/2,replace = T)
train
set.seed(1)
n=nrow(d1)
n/2
train=sample(n,n/2)
train
xtrain=x[train,]
ytrian=y[train]
m1 = lm(ytrian~xtrain)
m1
xtest=x[-train,]
ytest=y[-train]
yhat=predict(m1,xtest)
m1 = lm(hwfat~.,d1,subset = train)
m1
res = (hwfat-predict(m1,d1))[-train]^2
attach(d1)
res = (hwfat-predict(m1,d1))[-train]^2
res
mean(res)
cv.out=cv.glmnet(x[train,],y[train],alpha=0,folds=15)
11.27+5.02+2.99+2.99
cv.out=cv.glmnet(x[train,],y[train],alpha=0,K=15)
help("cv.glm")
cv.out=cv.glmnet(x[train,],y[train],alpha=0,K = 15)
cv.out=cv.glmnet(x[train,],y[train],alpha=0,nfolds=15)
cv.out$lambda
summary(cv.out)
cv.out$lambda.min
bestlam=cv.out$lambda.min
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[-train,])
mean((ridge.pred-y.test)^2)
ridge.pred=predict(ridge.mod,s=bestlam,x[-train,])
test=-train
test
test=(-train)
test
ytest=y[test]
bestlam=cv.out$lambda.min
ridge.pred=predict(ridge.mod,s=bestlam,newx=[test,])
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=bestlam)
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y[-train,])^2)
mean((ridge.pred-ytest)^2)
ytest=y[-train,]
test=-train
ytest=y[test,]
test=(-train)
ytest=y[test,]
mean((ridge.pred-y[-train])^2)
mean(res) #mspe
cv.out
summary(cv.out)
help("cv.glmnet")
plot(cv.out)
cv.out$lambda
cv.out=cv.glmnet(x[train,],y[train],alpha=1,nfolds=15)
summary(cv.out)
plot(cv.out)
bestlam=cv.out$lambda.min
ridge.mod=glmnet(x[train,],y[train],alpha=1,lambda=bestlam)
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y[-train])^2) #mspe 14.51665
bestlam
bestlam
cv.out=cv.glmnet(x[train,],y[train],alpha=0,nfolds=15)
summary(cv.out)
plot(cv.out)
bestlam=cv.out$lambda.min
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=bestlam)
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y[-train])^2) #mspe 14.51665
bestlam
